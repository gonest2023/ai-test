# name: my_ai_project # é¡¹ç›®åç§°
name: ${COMPOSE_PROJECT_NAME} # é¡¹ç›®åç§°

services:
  # Ollama æœåŠ¡ - AI æ¨¡å‹æœåŠ¡å™¨
  ollama-server:
    image: ollama/ollama:latest
    container_name: ollama-server  # ä¿æŒä¸æœåŠ¡åä¸€è‡´
    restart: unless-stopped
    
    # å¯é€‰ï¼šä¸ºæœåŠ¡æŒ‡å®šé…ç½®æ–‡ä»¶ï¼Œä¾¿äºåœ¨ä¸åŒç¯å¢ƒä¸‹ä½¿ç”¨
    # profiles:
    #   - ollama
    #   - production
    #   - development
    
    # æ•°æ®å·æŒ‚è½½
    volumes:
      # æ–¹å¼1ï¼šä½¿ç”¨å…·åå·ï¼ˆæ¨èï¼‰
      - ollama_data:/root/.ollama
      
      # æ–¹å¼2ï¼šä½¿ç”¨tmpfsï¼ˆå†…å­˜æ–‡ä»¶ç³»ç»Ÿï¼Œé‡å¯åæ•°æ®ä¸¢å¤±ï¼‰
      # - type: tmpfs
      #   target: /root/.ollama
      #   tmpfs:
      #     size: 4g
      #     mode: 777
      
      # æ–¹å¼3ï¼šåŒ¿åå·ç¤ºä¾‹
      # - /tmp
      
      # æ–¹å¼4ï¼šä¸»æœºç›®å½•æŒ‚è½½ç¤ºä¾‹ï¼ˆWindowsè·¯å¾„ï¼‰
      # - d:/custom_config:/config:ro
    
    entrypoint: ""   # ğŸ‘ˆ å…³é—­åŸæ¥çš„ /bin/ollama
    command: >
      sh -c "ollama serve &
             sleep 5 &&
             ollama pull gemma3:1b &&
             wait"
    # ollama serve & sleep 5 && ollama pull gemma3:1b && wait


    # ç½‘ç»œé…ç½®ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¼šåˆ›å»ºé¡¹ç›®ç½‘ç»œï¼‰

    networks:
      - ai_network
    
    # å¯é€‰ï¼šç›´æ¥æš´éœ²ç«¯å£ï¼ˆå¦‚æœéœ€è¦ä»å¤–éƒ¨è®¿é—®ï¼‰
    # ports:
    #   - "11434:11434"
    
    # èµ„æºé™åˆ¶
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 8G
        reservations:
          memory: 2G
    


  # Open WebUI - Ollamaçš„Webç•Œé¢
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    restart: unless-stopped
    
    # ç«¯å£æ˜ å°„
    ports:
      # - "8080:8080"  # é»˜è®¤ç«¯å£3000ï¼Œå¯é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–
      - "${APP_PORT:-3000}:8080"  # é»˜è®¤ç«¯å£3000ï¼Œå¯é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–

    # profiles:
    #   - open-webui
    #   - production
    
    # ç¯å¢ƒå˜é‡
    environment:
      - OLLAMA_BASE_URL=http://ollama-server:11434
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://ollama-server:11434}
      # - LOG_LEVEL=info
      # - WEBUI_AUTH=true
      # - WEBUI_SECRET_KEY=${WEBUI_SECRET_KEY:-change_this_secret_key}
    
    # æ•°æ®æŒä¹…åŒ–
    volumes:
      - openwebui_data:/app/backend/data
    
    # ç½‘ç»œé…ç½®
    networks:
      - ai_network
    
    # æœåŠ¡ä¾èµ–
    depends_on:
      - ollama-server
    
    # å¥åº·æ£€æŸ¥
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
    
    # èµ„æºé™åˆ¶
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 1G
 
# ç½‘ç»œé…ç½®
networks:
  ai_network:
    # driver: bridge
    # ipam:
    #   driver: default
    #   config:
    #     - subnet: 172.28.0.0/16

# æ•°æ®å·å£°æ˜
volumes:
  ollama_data:
    # external: true  # ä½¿ç”¨å¤–éƒ¨å·ï¼Œç¡®ä¿åœ¨ä¸åŒç¯å¢ƒä¸­æ•°æ®ä¸€è‡´æ€§
  openwebui_data:


# å¯é€‰ï¼šåŒ…å«å…¶ä»–é…ç½®æ–‡ä»¶
include:
  - nginx_app.yaml

# å¯é€‰ï¼šå¤–éƒ¨é…ç½®æ–‡ä»¶
# configs:
#   ollama_config:
#     file: ./ollama-config.json
 